Сховища даних, нормалізація, денормалізація даних

Завдання 1: Нормалізація до 2NF 
Перетворіть таблицю customers, щоб вона відповідала другій нормальній формі (2NF).
1.	Опишіть, які залежності є в таблиці.
2.	Які нові таблиці необхідно створити для досягнення 2NF?
3.	Подайте результат у вигляді нових таблиць.

Відповідь на завдання 1. Дослідження залежностей у таблиці customers:
У таблиці customers присутні такі залежності:
•	customer_id → customer_unique_id, customer_name, customer_city, customer_state
 
2. Нові таблиці для 2NF:
Щоб привести таблицю до 2NF, потрібно розділити таблицю customers на дві:
1.	customers: таблиця, що містить інформацію про клієнтів.
2.	customer_addresses: таблиця, що містить інформацію про адреси клієнтів.
3. Результат у вигляді нових таблиць:
customers:
customer_id	customer_unique_id	customer_name
1	abc123	Іван Петренко
2	def456	Олена Коваленко
customer_addresses:
customer_id	customer_city	customer_state
1	Київ	Київська
2	Одеса	Одеська
________________________________________
Завдання 2: Нормалізація до 3NF
Таблиця order_items має транзитивну залежність між полями. Перетворіть її так, щоб вона відповідала третій нормальній формі (3NF).
1.	Вкажіть транзитивні залежності, які потрібно усунути.
2.	Розбийте таблицю на кілька, щоб виконати умови 3NF.
3.	Опишіть, які зміни внесли і чому.
Відповідь до завдання 2: Нормалізація до 3NF
1. Транзитивні залежності:
У таблиці order_items може бути транзитивна залежність між полями order_id, product_id, quantity, price, де:
•	order_id → customer_id
•	product_id → price
 
2. Нові таблиці для 3NF:
Для усунення транзитивних залежностей розглянемо такі таблиці:
1.	order_items: таблиця, що містить інформацію про замовлення товарів.
2.	products: таблиця, що містить деталі товарів.
3. Опис змін:
•	У таблиці order_items видалено поле price, яке було перенесене до таблиці products.
•	Таким чином, утворюються дві нові таблиці:
order_items:
order_item_id	order_id	product_id	quantity
1	101	201	1
2	102	202	2
products:
product_id	product_name	product_category	price
201	Телефон	Електроніка	15000.00
202	Ноутбук	Комп'ютери	30000.00
________________________________________
Завдання 3: Денормалізація
Денормалізуйте таблиці orders та customers, щоб спростити отримання інформації про замовлення разом із даними клієнта.
1.	Об'єднайте дані з двох таблиць у нову денормалізовану таблицю.
2.	Продемонструйте кінцевий вигляд денормалізованої таблиці.
3.	Поясніть, які переваги та недоліки виникли після денормалізації


Відповідь на завдання 3: Денормалізація
1.	Об'єднання даних у денормалізовану таблицю:
 
 
Об'єднані дані з таблиць orders та customers можуть виглядати так:
denormalized_orders:
order_id	customer_id	customer_unique_id	customer_name	customer_city	customer_state	order_date	order_status
101	1	abc123	Іван Петренко	Київ	Київська	2024-01-01	delivered
102	2	def456	Олена Коваленко	Одеса	Одеська	2024-02-15	canceled
2. Кінцевий вигляд денормалізованої таблиці:
Денормалізовані дані подані в таблиці вище.
3. Переваги та недоліки денормалізації:
Переваги:
•	Спрощення запитів для отримання інформації про замовлення та клієнтів.
•	Зменшення кількості об'єднань у запитах.
Недоліки:
•	Зростання обсягу даних через повторення інформації.
•	Можливі проблеми з узгодженістю даних: зміна інформації про клієнта вимагає змін у багатьох місцях.
Ці зміни можуть полегшити отримання даних, але слід врахувати потенційні проблеми з керуванням даними у майбутньому.

Вибір архітектури зберігання даних на основі потреб бізнесу 
Завдання 4: Уявіть, що ви працюєте в компанії, яка має три різні підрозділи, і кожен з них має свої вимоги до зберігання та аналізу даних:
•	Підрозділ 1: Потрібно зберігати великі об'єми необроблених даних із зовнішніх джерел (лог файли, зображення, аудіо, відео, соціальні мережі).
•	Підрозділ 2: Потрібно зберігати структуровані дані про транзакції (покупки, оплати) для подальшого аналізу та звітності.
•	Підрозділ 3: Потрібно інтегрувати дані з різних джерел (підрозділ 1 і підрозділ 2) для комплексного аналізу і побудови моделей машинного навчання.
Для кожного підрозділу виберіть тип хранилища (Data Warehouse, Data Lake або Data Lakehouse), обґрунтувавши свій вибір. Поясніть, чому саме це хранилище найкраще відповідає вимогам кожного підрозділу.

________________________________________
Відповідь на завдання 4: Вибір архітектури зберігання даних
Підрозділ 1: Зберігання великих обсягів необроблених даних
Вибір: Data Lake
Обґрунтування:
•	Data Lake дозволяє зберігати великі об'єми різноманітних даних (лог-файли, зображення, аудіо, відео) в їхньому первісному форматі. Це зручно для експериментів і подальшого аналізу даних.
•	Зберігаючи дані "сирими", компанія може уникнути витрат на їх обробку до моменту використання.
________________________________________
Підрозділ 2: Зберігання структурованих даних про транзакції
Вибір: Data Warehouse
Обґрунтування:
•	Data Warehouse ідеально підходить для зберігання структурованих даних, таких як транзакції. Він забезпечує швидкий доступ до даних для аналізу та звітності.
•	Тут дані організовані у таблиці, що полегшує аналіз і генерацію звітів.
________________________________________
Підрозділ 3: Інтеграція даних з різних джерел
Вибір: Data Lakehouse
Обґрунтування:
•	Data Lakehouse поєднує переваги Data Lake і Data Warehouse. Він дозволяє зберігати як структуровані, так і необроблені дані в одному місці.
•	Це дозволяє підрозділу 3 інтегрувати дані з підрозділів 1 і 2 для комплексного аналізу та побудови моделей машинного навчання.

Завдання 5: Уявіть, що ваша компанія займається аналізом поведінки користувачів на вебсайті, зберіганням даних про транзакції та веде збір даних з мобільних додатків. Компанія має велику кількість даних різних типів:
•	Структуровані дані про транзакції (замовлення, оплати).
•	Неструктуровані дані (лог-файли вебсайтів, зображення, відео).
•	Полуструктуровані дані (дані з мобільних додатків у форматах JSON, XML).
Розробіть архітектуру для зберігання та обробки цих даних:
•	Як буде виглядати інтеграція між Data Warehouse, Data Lake та Data Lakehouse?
•	Які типи даних будуть зберігатися в кожному сховищі?
•	Як буде виглядати процес обробки даних для створення звітів і аналітики?
Окресліть можливі проблеми, з якими можна зіткнутися при реалізації такої архітектури, та запропонуйте шляхи їх вирішення.

Відповідь на завдання 5: Архітектура для зберігання та обробки даних
Інтеграція між сховищами:
•	Data Lake: Зберігає всі великі обсяги незструктурованих і полуструктурованих даних, таких як лог-файли, зображення та дані з мобільних додатків.
•	Data Warehouse: Зберігає структуровані дані про транзакції (наприклад, замовлення, оплати).
•	Data Lakehouse: Використовується для інтеграції даних з Data Lake і Data Warehouse. Забезпечує доступ до даних для аналітики та машинного навчання.
Типи даних у кожному сховищі:
•	Data Lake: Лог-файли вебсайтів, зображення, відео, дані з мобільних додатків (JSON, XML).
•	Data Warehouse: Структуровані дані про транзакції (замовлення, оплати).
•	Data Lakehouse: Інтегровані дані з обох попередніх сховищ для аналізу.
Процес обробки даних для звітів і аналітики:
1.	Витягнення: Дані витягуються з різних джерел, зберігаються в Data Lake.
2.	Перетворення: Дані очищуються, нормалізуються і агрегуються.
3.	Завантаження: Оброблені дані завантажуються в Data Warehouse для подальшого аналізу.
Можливі проблеми та шляхи вирішення:
•	Проблема: Взаємодія між різними системами може бути ускладнена.
•	Рішення: Використання стандартизованих API для інтеграції.
•	Проблема: Можливі помилки у даних через пропуски чи некоректні дані.
•	Рішення: Впровадження процедур для очищення і валідації даних.


Додаткове Завдання
 Розробити технічне завдання для налаштування ETL процесу, який буде використовуватися для інтеграції даних у Data Warehouse Опис ситуації: Компанія збирає дані з кількох джерел:
•	Система замовлень: інформація про транзакції (ID замовлення, дата, сума, статус, ID клієнта).
•	Система користувачів: інформація про користувачів (ID клієнта, ім’я, електронна пошта, дата реєстрації, статус).
•	Система продуктів: дані про продукти (ID продукту, назва, категорія, ціна).
•	Аналітичні дані: дані з аналітики поведінки користувачів на вебсайті (ID користувача, час проведений на сайті, сторінки переглянуті).
Завдання: Як Data Analyst, ваша задача - створити технічне завдання для процесу ETL для перенесення даних з цих систем в Data Warehouse. Вам необхідно:
•	Визначити джерела даних: описати кожне джерело даних, що буде використовуватися для ETL процесу.
•	Описати етапи ETL процесу:
•	Витягнення: Як буде відбуватися витягнення даних з різних джерел? Які інтерфейси або API будуть використовуватися для цього?
•	Трансформація: Які операції будуть проводитись для трансформації даних (наприклад, нормалізація, очищення, агрегація)? Як будуть оброблятися пропущені або некоректні дані?
•	Завантаження: Як дані будуть завантажуватися в Data Warehouse? Які таблиці будуть створюватися? Які дані будуть зберігатися в кожній таблиці?
Звичайно що ви можете не знати всі технічні складові процесів та інтеграцій, але писати словами так, як ви це розумієте.

Додаткове Завдання: Технічне завдання для ETL процесу
1. Джерела даних:
•	Система замовлень:
•	Інформація про транзакції (ID замовлення, дата, сума, статус, ID клієнта).
•	Система користувачів:
•	Інформація про користувачів (ID клієнта, ім’я, електронна пошта, дата реєстрації, статус).
•	Система продуктів:
•	Дані про продукти (ID продукту, назва, категорія, ціна).
•	Аналітичні дані:
•	Дані з аналітики поведінки користувачів на вебсайті (ID користувача, час проведений на сайті, сторінки переглянуті).
2. Етапи ETL процесу:
Витягнення:
•	Дані будуть витягуватись з бази даних за допомогою API чи SQL-запитів.
Трансформація:
•	Операції очищення даних (видалення дублікатів, перевірка формату).
•	Нормалізація даних (встановлення єдиного формату).
•	Агрегація даних для звітності (наприклад, підрахунок загальної суми замовлень за період).
Завантаження:
•	Дані будуть завантажені в Data Warehouse.
•	Створюються таблиці:
•	orders: ID замовлення, дата, сума, статус, ID клієнта.
•	customers: ID клієнта, ім’я, електронна пошта, дата реєстрації, статус.
•	products: ID продукту, назва, категорія, ціна.
•	user_behavior: ID користувача, час проведений на сайті, сторінки переглянуті.



